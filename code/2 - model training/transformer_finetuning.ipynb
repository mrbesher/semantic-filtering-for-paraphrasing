{"cells":[{"cell_type":"markdown","metadata":{"id":"c1W__GsXtsY3"},"source":["# Requirements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ph0AVSX1d5yK"},"outputs":[],"source":["!pip install datasets transformers sentencepiece torch tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rxBD68ZlGLdH"},"outputs":[],"source":["from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets, load_metric\n","from transformers import AutoTokenizer, AutoModelForConditionalGeneration, AdamW, get_scheduler\n","from functools import partial\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","from torch.cuda import is_available\n","from torch import device\n","from tqdm.auto import tqdm\n","import json\n","import random\n","import os"]},{"cell_type":"markdown","metadata":{"id":"aXR_Ul9Jt0T3"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jr-SCZW9q317"},"outputs":[],"source":["def train_test_split(dataset, test_ratio = 0.1, train_size = None, test_size = None):\n","    train, test = [data for _, data in dataset.train_test_split(test_ratio).items()]\n","    train = slice_if_available(train, train_size)\n","    test = slice_if_available(test, test_size)\n","    return train, test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3xaV3GYnW23"},"outputs":[],"source":["def slice_if_available(data, size):\n","    if size:\n","        if size < len(data):\n","            data = Dataset.from_dict(data[:size])\n","        else:\n","            raise Exception(f\"'size' must be smaller than len(data). size = {size}, len(data) = {len(data)}\")\n","\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mZsrQbxdz6ge"},"outputs":[],"source":["def get_dataloader(dataset, generate = False, max_length = 32, batch_size = 32, shuffle = True):\n","    dataset = dataset.map(partial(tokenize_seq2seq, generate = generate, max_length = max_length), batched = True)\n","    dataset = dataset.remove_columns([col for col in dataset.column_names if col not in ['input_ids', 'attention_mask', 'labels']])\n","    dataset.set_format(\"torch\")\n","    dataloader = DataLoader(dataset, batch_size, shuffle)\n","    return dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W5gzavVAIGKR"},"outputs":[],"source":["def tokenize_seq2seq(example, generate = False, max_length = 32, padding = \"max_length\", truncation = True):\n","    task_prefix = \"paraphrase: \" if \"mt5\" in architecture else \"\"\n","    if generate:\n","        tokenizer.padding_side = \"left\"\n","        tokenizer.pad_token = tokenizer.eos_token\n","    else:\n","        tokenizer.padding_side = \"right\"\n","        tokenizer.pad_token = \"<pad>\"\n","\n","    lower_map = {ord(u'I'): u'ı', ord(u'İ'): u'i'}\n","\n","    for i in range(len(example[\"src\"])):\n","        example[\"src\"][i] = task_prefix + example[\"src\"][i]\n","        if \"bart\" in architecture:\n","            example[\"src\"][i] = example[\"src\"][i].translate(lower_map).lower()\n","            example[\"tgt\"][i] = example[\"tgt\"][i].translate(lower_map).lower()\n","\n","    encoded_input = tokenizer(example[\"src\"], padding = padding, truncation = truncation, max_length = max_length)\n","    input_ids, attention_mask = encoded_input.input_ids, encoded_input.attention_mask\n","\n","    labels = tokenizer(example[\"tgt\"], padding = padding, truncation = truncation, max_length = max_length, return_tensors = \"pt\").input_ids\n","\n","    if not generate:\n","        labels[labels == tokenizer.pad_token_id] = CrossEntropyLoss().ignore_index\n","\n","    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels.tolist()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"siAB9XOF2YsT"},"outputs":[],"source":["def train(dataloader, num_epochs = 2):\n","    model.train()\n","    progress_bar = tqdm(range(num_training_steps // num_train_loops))\n","\n","    for _ in range(num_epochs):\n","        for batch in dataloader:\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            loss.backward()\n","\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            progress_bar.update(1)\n","\n","    progress_bar.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHkE3J3ApVmN"},"outputs":[],"source":["def test(dataloader):\n","    inputs, candidates, references = generate(dataloader)\n","    bleu_candidates, bleu_references = get_bleu_inputs(candidates, references)\n","    bleu = load_metric(\"bleu\")\n","    return inputs, candidates, references, bleu.compute(predictions = bleu_candidates, references = bleu_references)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FPQDzzkup4Ui"},"outputs":[],"source":["def generate(dataloader, max_length = 128, top_k = 50, top_p = 0.95, num_return_sequences = 1):\n","    inputs, candidates, references = [], [], []\n","\n","    progress_bar = tqdm(range(len(dataloader)))\n","\n","    for batch in dataloader:\n","        outputs = model.generate(\n","                input_ids = batch[\"input_ids\"].to(device),\n","                attention_mask = batch[\"attention_mask\"].to(device),\n","                do_sample = True,\n","                max_length = max_length,\n","                top_k = top_k,\n","                top_p = top_p,\n","                early_stopping = True,\n","                num_return_sequences = num_return_sequences)\n","\n","        inputs += [tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens = True, clean_up_tokenization_spaces = True)]\n","        candidates += [tokenizer.batch_decode(outputs, skip_special_tokens = True, clean_up_tokenization_spaces = True)]\n","        references += [tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens = True, clean_up_tokenization_spaces = True)]\n","\n","        progress_bar.update(1)\n","\n","    progress_bar.close()\n","\n","    return inputs, candidates, references"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfwZmTR1p7VD"},"outputs":[],"source":["def get_bleu_inputs(candidates, references):\n","    bleu_candidates, bleu_references = [], []\n","\n","    for candidate_batch, reference_batch in zip(candidates, references):\n","        for candidate, reference in zip(candidate_batch, reference_batch):\n","            bleu_candidates += [candidate.lower().split()]\n","            bleu_references += [[reference.lower().split()]]\n","\n","    return bleu_candidates, bleu_references"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sj0xYEO3SryQ"},"outputs":[],"source":["def generate_paraphrase(src, max_length = 128, num_return_sequences = 5, num_beams = 5):\n","  if \"mt5\" in architecture:\n","    src = 'paraphrase: ' + src\n","\n","  tokenized = tokenizer.encode_plus(src, return_tensors='pt')\n","  tokenized.to(device)\n","\n","  outputs = model.generate(tokenized['input_ids'], max_length = max_length, num_return_sequences = num_return_sequences, num_beams = num_beams)\n","\n","  return tokenizer.batch_decode(outputs, skip_special_tokens=True)"]},{"cell_type":"markdown","metadata":{"id":"g2AIq64WuCjd"},"source":["# Arguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zLgGAIdFLVx"},"outputs":[],"source":["dataset = 'name_of_dataset'\n","architecture = \"model_architecture\" # mt5-base, mt5-small, bart\n","model_checkpoint = 'model_checkpoint' # google/mt5-base, mukayese/bart-base-turkish-sum\n","root = '/path/to/project'\n","starting_epoch = 0\n","num_train_loops = 2\n","num_epochs = 1\n","lr = 1e-4"]},{"cell_type":"markdown","metadata":{"id":"D-BxLqmEt-jO"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99SLi3xQHlKZ"},"outputs":[],"source":["data = load_dataset(\"csv\", data_files = {\"train\": f\"{root}/path/to/train_dataset.csv\",\n","                                         \"test\": f\"{root}/path/to/test_dataset.csv\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRzQGKvNGOqS"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-VRrQCCII1QF"},"outputs":[],"source":["train_dataloader = get_dataloader(data[\"train\"])\n","test_dataloader = get_dataloader(data[\"test\"], generate = True, shuffle = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KwcdIWGkcGk1"},"outputs":[],"source":["device = device(\"cuda\") if is_available() else device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbmKcVxzZQSM"},"outputs":[],"source":["model_checkpoint = f'{root}/models/{architecture}-{dataset}/model-{starting_epoch}' if starting_epoch else model_checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmJxe_DMIOOe"},"outputs":[],"source":["model = AutoModelForConditionalGeneration.from_pretrained(model_checkpoint).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGMH2NWQTG6X"},"outputs":[],"source":["optimizer = AdamW(model.parameters(), lr = lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKflie0uhak1"},"outputs":[],"source":["num_training_steps = num_train_loops * num_epochs * len(train_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWY_LvQ05RUI"},"outputs":[],"source":["prefix_length = len(\"paraphrase: \") if \"mt5\" in architecture else 0\n","results = []\n","for i in range(num_train_loops):\n","  train(train_dataloader, num_epochs = num_epochs)\n","\n","  foldername = f'{root}/models/{architecture}-{dataset}/model-{starting_epoch + num_epochs*(i+1)}'\n","\n","  model.save_pretrained(foldername)\n","  result = test(test_dataloader)\n","  results.append(result)\n","  os.makedirs(foldername, exist_ok=True)\n","  with open(f'{foldername}/result.json', 'w', encoding='utf-8') as f:\n","    sample = random.choice(list(zip(result[0], result[1], result[2])))\n","    data = {'score': result[-1],\n","            'data': [{'i': sample[0][i][prefix_length:], 'c': sample[1][i], 'r': sample[-1][i]} for i in range(len(sample[0]))]}\n","    json.dump(data, f, ensure_ascii=False, indent=4)"]},{"cell_type":"markdown","metadata":{"id":"08fmwgUYwjnq"},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4-hsaXjr5Is"},"outputs":[],"source":["generate_paraphrase(\"Erkek işini yapmak için çocuk yollayamazsın.\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"1BBNmkpKF6qDO-RzZafSnWhBLmBTgOAWZ","timestamp":1630136330789}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
