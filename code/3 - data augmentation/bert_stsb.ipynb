{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PyO4ZpJQx-eU"},"outputs":[],"source":["!pip install --quiet -U sentence_transformers datasets torch pandas"]},{"cell_type":"code","source":["import gdown\n","import torch\n","import pandas as pd\n","\n","from pathlib import Path\n","from datetime import datetime\n","from torch.utils.data import DataLoader\n","from torch.cuda import is_available\n","from datasets import load_dataset\n","from sentence_transformers import SentenceTransformer\n","from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n","from sentence_transformers.cross_encoder import CrossEncoder\n","from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n","from sentence_transformers import InputExample, losses, models"],"metadata":{"id":"G1M7MA5-1QYX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparameters\n","train_batch_size = 32\n","num_epochs = 4\n","n_trainings = 5\n","sample_ratio = 0.05\n","augment = True\n","\n","# arguments\n","model_checkpoint = 'dbmdz/bert-base-turkish-cased'\n","model_save_path = Path('model_checkpoint')\n","current_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n","if augment:\n","  results_path = Path(f'/path/to/results')\n","else:\n","  num_epochs *= 2\n","  results_path = Path(f'/path/to/results')"],"metadata":{"id":"p5hBU_mh9qTG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_path.mkdir(parents=True, exist_ok=True)\n","model_save_path.mkdir(parents=True, exist_ok=True)"],"metadata":{"id":"UsjeSYZtXV_u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Download Model and Dataset"],"metadata":{"id":"c8-0P_lO8Twz"}},{"cell_type":"markdown","source":["## STS-B"],"metadata":{"id":"d-6pUNTUAphQ"}},{"cell_type":"code","source":["dataset_path = Path('/path/to/dataset')"],"metadata":{"id":"571GBX_71Rjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_train_df = pd.read_csv(dataset_path / 'train_file_name.csv')\n","dev = pd.read_csv(dataset_path / 'dev_file_name.csv')\n","test = pd.read_csv(dataset_path / 'test_file_name.csv')\n","\n","# # normalize score between [0-1]\n","raw_train_df['score'] = raw_train_df['score'] / 5\n","dev['score'] = dev['score'] / 5\n","test['score'] = test['score'] / 5"],"metadata":{"id":"_XW1N_2x7eQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = raw_train_df.sample(int(len(raw_train_df) * sample_ratio), random_state = 42)"],"metadata":{"id":"ZK1yK6flWRNd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.head()"],"metadata":{"id":"ybEFEneSX78w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dev_samples = [InputExample(texts=[row['sentence1'], row['sentence2']], label=row['score']) for _, row in dev.iterrows()]\n","test_samples = [InputExample(texts=[row['sentence1'], row['sentence2']], label=row['score']) for _, row in test.iterrows()]\n","train_samples = [InputExample(texts=[row['sentence1'], row['sentence2']], label=row['score']) for _, row in train.iterrows()]"],"metadata":{"id":"_2JyUDQsU52U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if augment:\n","  train_samples = train_samples + [InputExample(texts=[row['sentence1_paraphrase'], row['sentence2_paraphrase']], label=row['score']) for _, row in train.iterrows()]"],"metadata":{"id":"kJPWnlQUX1vS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"Mi1rlwlh9t89"}},{"cell_type":"code","source":["device = torch.device('cuda') if is_available() else torch.device('cpu')"],"metadata":{"id":"tQBPpjGCLb9O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We wrap train_samples (which is a List[InputExample]) into a pytorch DataLoader\n","train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n","warmup_steps = int(len(train_dataloader) * num_epochs * 0.1 + .5) #10% of train data for warm-up\n","\n","for i in range(n_trainings):\n","  dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name=f'berturk-stsb-dev-{i+1}')\n","  test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name=f'berturk-stsb-test{i+1}')\n","\n","  # STS-b\n","  model = SentenceTransformer(model_checkpoint, device=device)\n","  train_loss = losses.CosineSimilarityLoss(model=model)\n","  model.fit(train_objectives=[(train_dataloader, train_loss)],\n","          evaluator=dev_evaluator,\n","          epochs=num_epochs,\n","          warmup_steps=warmup_steps,\n","          output_path=str(model_checkpoint))\n","\n","  dev_evaluator(model, output_path=results_path)\n","  test_evaluator(model, output_path=results_path)"],"metadata":{"id":"MHcs53GbWL0Z"},"execution_count":null,"outputs":[]}]}